{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "config = {\n",
    "    'host': '127.0.0.1',\n",
    "    'user': 'root',\n",
    "    'password': 'xuhengda',\n",
    "    'database': 'BOOSTDB',\n",
    "}\n",
    "cnx = mysql.connector.connect(**config)\n",
    "print(cnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端] 读数据\n",
    "* 读UTTERANCES表\n",
    "* 写CONVS表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_id = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cnx.cursor()\n",
    "\n",
    "sql = 'SELECT CONV_ID, UTTERANCE FROM UTTERANCES WHERE BOT_ID = %s'\n",
    "data = (bot_id,)\n",
    "cursor.execute(sql, data)\n",
    "\n",
    "corpus = []\n",
    "conv = ''\n",
    "last_conv_id = None\n",
    "for conv_id, utterance in cursor:\n",
    "    if conv_id != last_conv_id:\n",
    "        corpus.append(conv[:len(conv) - 1])\n",
    "        conv = ''\n",
    "        last_conv_id = conv_id\n",
    "    else:\n",
    "        conv += utterance + ' '\n",
    "corpus.append(conv[:len(conv) - 1])\n",
    "corpus = corpus[1:]\n",
    "cursor.close()\n",
    "\n",
    "print(len(corpus), 'conversations', end='\\n\\n')\n",
    "print(corpus[0])\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "sql = 'SELECT DISTINCT CONV_ID FROM UTTERANCES WHERE BOT_ID = %s'\n",
    "data = (bot_id,)\n",
    "cursor.execute(sql, data)\n",
    "conv_ids = list(map(lambda t: int(t[0]), cursor))\n",
    "\n",
    "sql = 'INSERT INTO CONVS (CONV_ID, BOT_ID) VALUES '\n",
    "sql_value = '(%s, %s), ' * len(conv_ids)\n",
    "sql += sql_value[:-2]\n",
    "data = []\n",
    "for conv_id in conv_ids:\n",
    "    data.append(conv_id)\n",
    "    data.append(bot_id)\n",
    "cursor.execute(sql, data)\n",
    "cnx.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端] 数据清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "\n",
    "jieba.setLogLevel(logging.INFO)\n",
    "logging.basicConfig(format='%(funcName)s:%(message)s', level=logging.DEBUG)\n",
    "\n",
    "stop_wrods_file = 'stop_words.txt'\n",
    "\n",
    "def _substitute(sent):\n",
    "    exps = [\n",
    "        r'#E-\\w\\[数字x\\]|~O\\(∩_∩\\)O/~',\n",
    "        r'http[s]?://[a-zA-Z0-9|\\.|/]+',\n",
    "        r'http[s]?://[a-zA-Z0-9\\./-]*\\[链接x\\]',\n",
    "        r'\\[ORDERID_[0-9]+\\]',\n",
    "        r'\\[日期x\\]',\n",
    "        r'\\[时间x\\]',\n",
    "        r'\\[金额x\\]',\n",
    "        r'\\[站点x\\]',\n",
    "        r'\\[数字x\\]',\n",
    "        r'\\[地址x\\]',\n",
    "        r'\\[姓名x\\]',\n",
    "        r'\\[邮箱x\\]',\n",
    "        r'\\[电话x\\]',\n",
    "        r'\\[商品快照\\]',\n",
    "        r'<s>',\n",
    "        r'\\s+',\n",
    "        r'[a-z|0-9]+'\n",
    "        \"[\\s+\\.\\!\\/_,$%^:*(+\\\"\\')]+\",\n",
    "        \"[+——()?:【】‘’“”`！，。？、~@#￥%……&*（）]+\"\n",
    "    ]\n",
    "    for exp in exps:\n",
    "        sent = re.sub(exp, ' ', sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "logging.info('数据清洗开始...')\n",
    "\n",
    "# 正则表达式替换特定字符串\n",
    "corpus = list(map(_substitute, corpus))\n",
    "logging.info('正则表达式替换完成.')\n",
    "\n",
    "# 分词\n",
    "t = time.time()\n",
    "corpus = list(map(jieba.cut, corpus))\n",
    "logging.info('分词完成.')\n",
    "\n",
    "# 删除停用词\n",
    "logging.info('删除停用词开始...')\n",
    "with open(stop_wrods_file, encoding='utf-8') as f:\n",
    "    stop_words = f.read().strip().split('\\n')\n",
    "\n",
    "t = time.time()\n",
    "for i in range(len(corpus)):\n",
    "    tokens = []\n",
    "    for token in corpus[i]:\n",
    "        token = token.strip()\n",
    "        if len(token) > 1 and token not in stop_words:\n",
    "            tokens.append(token)\n",
    "    corpus[i] = tokens\n",
    "logging.info('删除停用词完成 (用时: %.2fs).' % (time.time() - t))\n",
    "\n",
    "# 组合\n",
    "corpus = list(map(lambda x: ' '.join(x), corpus))\n",
    "\n",
    "logging.info('数据清洗完成.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端] TFIDF\n",
    "* 写WORDS表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "max_df = 0.1\n",
    "min_df = 20\n",
    "max_features = 1000\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=max_df,\n",
    "                                   min_df=min_df,\n",
    "                                   max_features=max_features)\n",
    "tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 获取每个词的df值\n",
    "idf = tfidf_vectorizer.idf_\n",
    "df = list(map(lambda x: (len(corpus) + 1) / np.exp(x - 1) - 1, idf))\n",
    "df = list(map(lambda x: x / len(corpus), df))\n",
    "\n",
    "words = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "sql = 'INSERT INTO WORDS (WORD_ID, BOT_ID, WORD, DF) VALUES '\n",
    "sql_values = '(%s, %s, %s, %s), ' * len(words)\n",
    "sql += sql_values[:-2]\n",
    "data = []\n",
    "for (word_id, (word, df)) in enumerate(zip(words, df)):\n",
    "    data.extend([word_id, bot_id, word, df])\n",
    "cursor.execute(sql, data)\n",
    "cnx.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端] 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 50 # 参数\n",
    "max_iter=100\n",
    "\n",
    "kmeans = KMeans(n_clusters=n_clusters,\n",
    "                    max_iter=max_iter,\n",
    "                    n_init=8,\n",
    "                    init='k-means++',\n",
    "                    n_jobs=-1,\n",
    "                    random_state=0,\n",
    "                    verbose=1)\n",
    "labels = kmeans.fit_predict(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端]  计算CH指数\n",
    "* 写BOTS表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "ch = metrics.calinski_harabaz_score(tfidf.toarray(), labels)\n",
    "print('CH: %.3f' % ch)\n",
    "\n",
    "cursor = cnx.cursor()\n",
    "sql = 'UPDATE BOTS SET CH = %s WHERE BOT_ID = %s'\n",
    "data = (ch, bot_id)\n",
    "cursor.execute(sql, data)\n",
    "cnx.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端] 记录每段对话写类别标签\n",
    "* 写CONVS表\n",
    "* 多次UPDATE（待优化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cnx.cursor()\n",
    "sql = 'UPDATE CONVS SET CLUSTER_ID = %s WHERE BOT_ID = %s AND CONV_ID = %s'\n",
    "for conv_id, cluster_id in enumerate(labels):\n",
    "    data = (int(cluster_id), bot_id, conv_id)\n",
    "    cursor.execute(sql, data)\n",
    "cnx.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端] 记录每个簇的对话数量\n",
    "* 写CLUSTERS表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [(labels == idx).sum() for idx in range(n_clusters)]\n",
    "cursor = cnx.cursor()\n",
    "sql = 'INSERT INTO CLUSTERS (CLUSTER_ID, BOT_ID, VOLUME) VALUES'\n",
    "sql_values = ' (%s, %s, %s),' * n_clusters\n",
    "sql += sql_values[:-1]\n",
    "data = []\n",
    "for cluster_id, volume in enumerate(counts):\n",
    "    data.extend([cluster_id, bot_id, int(volume)])\n",
    "cursor.execute(sql, data)\n",
    "cnx.commit()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [后端] 计算每个簇的关键词\n",
    "* 写CLUSTER_KEYWORDS表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cnx.cursor()\n",
    "\n",
    "td = list(map(lambda x: x.toarray().squeeze(), tfidf))\n",
    "    \n",
    "vectorss = [[] for _ in range(n_clusters)]\n",
    "for i in range(len(corpus)):\n",
    "    vectorss[labels[i]].append(td[i])\n",
    "    \n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "for cluster_id, vectors in enumerate(vectorss):\n",
    "    vectors = np.array(vectors)\n",
    "    vectors = vectors.sum(0)\n",
    "    num = (vectors != 0.0).sum()\n",
    "    word_ids = vectors.argsort()[:-1-num:-1]\n",
    "    \n",
    "    sql = 'INSERT INTO CLUSTER_KEYWORDS (BOT_ID, CLUSTER_ID, WORD_ID, TFIDF) VALUES'\n",
    "    sql_values = ' (%s, %s, %s, %s),' * len(word_ids)\n",
    "    sql += sql_values[:-1]\n",
    "    data = []\n",
    "    for word_id in word_ids:\n",
    "        data.extend((bot_id, cluster_id, int(word_id), float(vectors[word_id])))\n",
    "    cursor.execute(sql, data)\n",
    "cnx.commit()\n",
    "cursor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
