{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "\n",
    "class Database:\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.cnx = mysql.connector.connect(**config)\n",
    "        \n",
    "    def new_bot(self, bot_id):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'INSERT INTO CLUSTERING_STATUS (BOT_ID) VALUES (%s)'\n",
    "        data = (bot_id,)\n",
    "        cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def query_progress(self, bot_id):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'SELECT IS_COMPLETED, PROGRESS FROM CLUSTERING_STATUS WHERE BOT_ID = %s'\n",
    "        data = (bot_id,)\n",
    "        cursor.execute(sql, data)\n",
    "        is_completed = False\n",
    "        progress = 0.0\n",
    "        for c, p in cursor:\n",
    "            is_completed = True if int(c) == 1 else False\n",
    "            progress = float(p)\n",
    "        cursor.close()\n",
    "        return is_completed, progress\n",
    "    \n",
    "    def update_progress(self, bot_id, is_completed, progress):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = ('UPDATE CLUSTERING_STATUS SET IS_COMPLETED = %s, PROGRESS = %s '\n",
    "               'WHERE BOT_ID = %s')\n",
    "        data = (is_completed, progress, bot_id)\n",
    "        cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    def insert_conv(self, bot_id, conv_id, conv):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'INSERT INTO CONVS (CONV_ID, BOT_ID, CONV) VALUES (%s, %s, %s)'\n",
    "        data = (conv_id, bot_id, conv)\n",
    "        cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def insert_convs(self, bot_id, conversations):\n",
    "        cursor = self.cnx.cursor()\n",
    "        for conv_id, conv in conversations:\n",
    "            sql = 'INSERT INTO CONVS (CONV_ID, BOT_ID, CONV) VALUES (%s, %s, %s)'\n",
    "            data = (conv_id, bot_id, conv)\n",
    "            cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def select_all_convs(self, bot_id):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'SELECT CONV FROM CONVS WHERE BOT_ID = %s'\n",
    "        data = (bot_id,)\n",
    "        cursor.execute(sql, data)\n",
    "        corpus = [row[0] for row in cursor]\n",
    "        cursor.close()\n",
    "        return corpus\n",
    "    \n",
    "    def insert_word(self, bot_id, word_id, word):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = ('INSERT INTO WORDS (WORD_ID, BOT_ID, WORD)'\n",
    "               'VALUES (%s, %s, %s)')\n",
    "        data = (word_id, bot_id, word)\n",
    "        cursor.execute(sql)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def insert_words(self, bot_id, words):\n",
    "        cursor = self.cnx.cursor()\n",
    "        for word_id, word in words:\n",
    "            sql = 'INSERT INTO WORDS (WORD_ID, BOT_ID, WORD) VALUES (%s, %s, %s)'\n",
    "            data = (word_id, bot_id, word)\n",
    "            cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def insert_words_with_df(self, bot_id, words):\n",
    "        cursor = self.cnx.cursor()\n",
    "        for word_id, word, df in words:\n",
    "            sql = 'INSERT INTO WORDS (WORD_ID, BOT_ID, WORD, DF) VALUES (%s, %s, %s, %s)'\n",
    "            data = (word_id, bot_id, word, df)\n",
    "            cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def select_all_words(self, bot_id):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'SELECT WORD FROM WORDS WHERE BOT_ID = %s'\n",
    "        data = (bot_id,)\n",
    "        cursor.execute(sql, data)\n",
    "        words = [row[0] for row in cursor]\n",
    "        cursor.close()\n",
    "        return words\n",
    "    \n",
    "    def insert_dfs(self, bot_id, dfs):\n",
    "        cursor = self.cnx.cursor()\n",
    "        for word_id, df in dfs:\n",
    "            sql = ('UPDATE WORDS SET DF = %s'\n",
    "                   'WHERE BOT_ID = %s AND WORD_ID = %s')\n",
    "            data = (df, bot_id, word_id)\n",
    "            cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def select_all_dfs(self, bot_id):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'SELECT DF FROM WORDS WHERE BOT_ID = %s'\n",
    "        data = (bot_id,)\n",
    "        cursor.execute(sql, data)\n",
    "        dfs = [float(row[0]) for row in cursor]\n",
    "        cursor.close()\n",
    "        return dfs\n",
    "    \n",
    "    def insert_tfidfs(self, bot_id, tfidfs):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'INSERT INTO TFIDFS (TFIDF_ID, BOT_ID, CONV_ID, WORD_ID, TFIDF) VALUES'\n",
    "        tfidf_id = 0\n",
    "        for conv_id, row in enumerate(tfidfs):\n",
    "            for word_id, tfidf in enumerate(row):\n",
    "                sql = 'INSERT INTO TFIDFS (TFIDF_ID, BOT_ID, CONV_ID, WORD_ID, TFIDF) VALUES'\n",
    "                sql += ' (%s, %s, %s, %s, %s)'\n",
    "                data = (tfidf_id, bot_id, conv_id, word_id, tfidf)\n",
    "                tfidf_id += 1\n",
    "                cursor.execute(sql, data)\n",
    "        self.cnx.commit()\n",
    "        cursor.close()\n",
    "        \n",
    "    def select_all_tfidfs(self, bot_id):\n",
    "        cursor = self.cnx.cursor()\n",
    "        sql = 'SELECT CONV_ID, WORD_ID, TFIDF FROM TFIDFS WHERE BOT_ID = %s'\n",
    "        data = (bot_id,)\n",
    "        cursor.execute(sql, data)\n",
    "        tfidfs = [[conv_id, word_id, float(tfidf)] for conv_id, word_id, tfidf in cursor]\n",
    "        cursor.close()\n",
    "        return tfidfs\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.cnx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 服务器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bottle v0.13-dev server starting up (using WSGIRefServer(ip='127.0.0.1'))...\n",
      "Listening on http://127.0.0.1:8080/\n",
      "Hit Ctrl-C to quit.\n",
      "\n",
      "data_cleaning:数据清洗开始...\n",
      "127.0.0.1 - - [24/Oct/2018 16:07:20] \"POST / HTTP/1.1\" 200 11\n",
      "data_cleaning:正则表达式替换完成.\n",
      "data_cleaning:分词完成.\n",
      "data_cleaning:删除停用词开始...\n",
      "data_cleaning:删除停用词完成 (用时: 23.39s).\n",
      "data_cleaning:数据清洗完成.\n",
      "Exception in thread Thread-34:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dada/anaconda3/lib/python3.6/site-packages/mysql/connector/connection_cext.py\", line 392, in cmd_query\n",
      "    raw_as_string=raw_as_string)\n",
      "_mysql_connector.MySQLInterfaceError: Duplicate entry '0' for key 'PRIMARY'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dada/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"<ipython-input-73-84a543b53b3b>\", line 47, in run\n",
      "    self.convert_tfidf()\n",
      "  File \"<ipython-input-73-84a543b53b3b>\", line 133, in convert_tfidf\n",
      "    self.db.insert_words_with_df(self.bot_id, words)\n",
      "  File \"<ipython-input-68-a3a7dee5e5f3>\", line 89, in insert_words_with_df\n",
      "    cursor.execute(sql, data)\n",
      "  File \"/Users/dada/anaconda3/lib/python3.6/site-packages/mysql/connector/cursor_cext.py\", line 266, in execute\n",
      "    raw_as_string=self._raw_as_string)\n",
      "  File \"/Users/dada/anaconda3/lib/python3.6/site-packages/mysql/connector/connection_cext.py\", line 395, in cmd_query\n",
      "    sqlstate=exc.sqlstate)\n",
      "mysql.connector.errors.IntegrityError: 1062 (23000): Duplicate entry '0' for key 'PRIMARY'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bottle\n",
    "import json\n",
    "import threading\n",
    "\n",
    "\n",
    "class Server:\n",
    "    \n",
    "    def __init__(self, ip='127.0.0.1', port=8080, debug=False):\n",
    "        self.ip = ip\n",
    "        self.port = port\n",
    "        self.debug = debug,\n",
    "\n",
    "    @bottle.post('/')\n",
    "    def cluster():\n",
    "        data = bottle.request.json\n",
    "        bot_id = data['bot_id']\n",
    "        convs = data['convs']\n",
    "        stop_words = data['stop_words']\n",
    "        max_df = data['max_df']\n",
    "        min_df = data['min_df']\n",
    "        n_clusters = data['n_clusters']\n",
    "        method = data['method']\n",
    "        cluster_thread = ClusterThread(\n",
    "            bot_id, convs, stop_words, max_df, min_df, n_clusters, method)\n",
    "        cluster_thread.start()\n",
    "        return 'successfull'\n",
    "        \n",
    "    def start(self):\n",
    "        bottle.run(ip=self.ip, port=self.port, debug=self.debug)\n",
    "        \n",
    "        \n",
    "Server().start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<input>:70: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:70: DeprecationWarning: invalid escape sequence \\s\n",
      "<input>:70: DeprecationWarning: invalid escape sequence \\s\n",
      "<ipython-input-73-84a543b53b3b>:70: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"[\\s+\\.\\!\\/_,$%^:*(+\\\"\\')]+\",\n",
      "pylab_setup:backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import logging\n",
    "import time\n",
    "import threading\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class ClusterThread(threading.Thread):\n",
    "    \n",
    "    def __init__(self, bot_id, convs, stop_words, max_df, min_df, n_clusters, method):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.bot_id = bot_id\n",
    "        self.convs = convs\n",
    "        self.stop_words = stop_words\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.n_clusters = n_clusters\n",
    "        self.method = method\n",
    "        self.max_features = 1000\n",
    "        \n",
    "        self.raw_file = 'chat-short-20w.txt'\n",
    "        self.stop_wrods_file = 'stop_words.txt'\n",
    "\n",
    "        jieba.setLogLevel(logging.INFO)\n",
    "        logging.basicConfig(format='%(funcName)s:%(message)s', level=logging.DEBUG)\n",
    "        \n",
    "        self.corpus = []\n",
    "        self.tfidf = None\n",
    "        \n",
    "        self.db_config = {\n",
    "          'host': '127.0.0.1',\n",
    "          'user': 'root',\n",
    "          'password': 'xuhengda',\n",
    "          'database': 'TEST',\n",
    "        }\n",
    "        self.db = Database(self.db_config)\n",
    "        \n",
    "    def run(self):\n",
    "        self.db.new_bot(self.bot_id)\n",
    "#         time.sleep(4)\n",
    "        self.data_cleaning()\n",
    "        self.convert_tfidf()\n",
    "        logging.info('Finish.')\n",
    "        self.db.update_progress(self.bot_id, 1, 1.0)\n",
    "        \n",
    "    def _substitute(self, sent):\n",
    "        exps = [\n",
    "            r'#E-\\w\\[数字x\\]|~O\\(∩_∩\\)O/~',\n",
    "            r'http[s]?://[a-zA-Z0-9|\\.|/]+',\n",
    "            r'http[s]?://[a-zA-Z0-9\\./-]*\\[链接x\\]',\n",
    "            r'\\[ORDERID_[0-9]+\\]',\n",
    "            r'\\[日期x\\]',\n",
    "            r'\\[时间x\\]',\n",
    "            r'\\[金额x\\]',\n",
    "            r'\\[站点x\\]',\n",
    "            r'\\[数字x\\]',\n",
    "            r'\\[地址x\\]',\n",
    "            r'\\[姓名x\\]',\n",
    "            r'\\[邮箱x\\]',\n",
    "            r'\\[电话x\\]',\n",
    "            r'\\[商品快照\\]',\n",
    "            r'<s>',\n",
    "            r'\\s+',\n",
    "            r'[a-z|0-9]+'\n",
    "            \"[\\s+\\.\\!\\/_,$%^:*(+\\\"\\')]+\",\n",
    "            \"[+——()?:【】‘’“”`！，。？、~@#￥%……&*（）]+\"\n",
    "        ]\n",
    "        for exp in exps:\n",
    "            sent = re.sub(exp, ' ', sent)\n",
    "        return sent\n",
    "        \n",
    "    def data_cleaning(self):\n",
    "        logging.info('数据清洗开始...')\n",
    "        self.corpus = self.db.select_all_convs(self.bot_id)\n",
    "\n",
    "        # 正则表达式替换特定字符串\n",
    "        self.corpus = list(map(self._substitute, self.corpus))\n",
    "        logging.info('正则表达式替换完成.')\n",
    "\n",
    "        # 分词\n",
    "        t = time.time()\n",
    "        self.corpus = list(map(jieba.cut, self.corpus))\n",
    "        logging.info('分词完成.')\n",
    "\n",
    "        # 删除停用词\n",
    "        # TODO: 需要读数据库吗？\n",
    "        logging.info('删除停用词开始...')\n",
    "        with open(self.stop_wrods_file, encoding='utf-8') as f:\n",
    "            stop_words = f.read().strip().split('\\n')\n",
    "\n",
    "        self.stop_words.extend(stop_words)\n",
    "\n",
    "        t = time.time()\n",
    "        for i in range(len(self.corpus)):\n",
    "            tokens = []\n",
    "            for token in self.corpus[i]:\n",
    "                token = token.strip()\n",
    "                if len(token) > 1 and token not in self.stop_words:\n",
    "                    tokens.append(token)\n",
    "            self.corpus[i] = tokens\n",
    "        logging.info('删除停用词完成 (用时: %.2fs).' % (time.time() - t))\n",
    "\n",
    "        # 组合\n",
    "        self.corpus = list(map(lambda x: ' '.join(x), self.corpus))\n",
    "\n",
    "        logging.info('数据清洗完成.')\n",
    "                \n",
    "    def convert_tfidf(self):\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_df=self.max_df,\n",
    "                                           min_df=self.min_df,\n",
    "                                           max_features=self.max_features)\n",
    "        self.tfidf = tfidf_vectorizer.fit_transform(self.corpus)\n",
    "        \n",
    "        # 获取每个词的df值\n",
    "        # TODO: 写数据库, tfidf: 2维, df: 1维\n",
    "        idf = tfidf_vectorizer.idf_\n",
    "        df = list(map(lambda x: (len(self.corpus) + 1) / np.exp(x - 1) - 1, idf))\n",
    "        df = list(map(lambda x: x / len(self.corpus), df))\n",
    "#         df.sort()\n",
    "        \n",
    "        terms = tfidf_vectorizer.get_feature_names()\n",
    "        words = []\n",
    "        for idx, word in enumerate(terms):\n",
    "            words.append([idx, word])\n",
    "            \n",
    "        for i in range(len(words)):\n",
    "            words[i].append(df[i])\n",
    "        self.db.insert_words_with_df(self.bot_id, words)\n",
    "\n",
    "        # TODO: 前端读数据库\n",
    "        print('文件频率df-词数')\n",
    "        plt.hist(df, 200)\n",
    "        plt.show()\n",
    "\n",
    "        print('每个词的文件频率df')\n",
    "        plt.bar(np.arange(len(df)), df)\n",
    "        plt.show()\n",
    "        \n",
    "        # 将每段对话中的词按tfidf值从高到低排序\n",
    "        # TODO: 前端显示总词数\n",
    "        print('total words:', len(tfidf_vectorizer.vocabulary_), end='\\n\\n')\n",
    "        \n",
    "        # 写数据库tfidf\n",
    "        logging.info('开始写入tfidf值...')\n",
    "        self.db.insert_tfidfs(self.bot_id, self.tfidf.toarray())\n",
    "        logging.info('写入tfidf值完成.')\n",
    "\n",
    "        # 打印指定对话中的关键词\n",
    "        # TODO: 前端\n",
    "        terms = tfidf_vectorizer.get_feature_names()\n",
    "        conv_idx = 0\n",
    "        for row in self.tfidf[:4]:\n",
    "            conv_idx += 1\n",
    "            print('Conv %d: ' % conv_idx, end='')\n",
    "            row = row.toarray().squeeze()\n",
    "            num = min(30, (row != 0).sum())\n",
    "            indexes = row.argsort()[:-1-num:-1]\n",
    "            words = [terms[idx] for idx in indexes]\n",
    "            print(' '.join(words), end='\\n\\n')\n",
    "\n",
    "            values = [row[idx] for idx in indexes]\n",
    "            plt.bar(range(len(values)), values)\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
